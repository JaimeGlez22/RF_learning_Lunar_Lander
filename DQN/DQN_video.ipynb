{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parte 2 1ª Practica: implementación DQN LunarLander\n",
        "\n",
        "Este trabajo ha sido realizado por Adrián Contreras Castillo y Jaime González Delgado. Donde se busca implementar un modelo DQN para jugar al juego LunarLander, donde se busca aterrizar una nave en la superficie. Para ello se cuentan con 8 variables que sirven como input para la red. Mientras que los posibles movimientos que puede realizar la nave son 4, no hacer nada, encender motor izquierdo, encender motor principal o encender motor derecho.\n",
        "\n",
        "El entrenamiento se ha realizado en la plataforma de Google Colab haciendo uso de las GPUs T4, pero una vez que se ha entrenado se puede cargar el modelo y ejecutarla en un ordenador sin GPU o sin la opción CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWpvr7Grdin",
        "outputId": "00d73edf-f1dc-464c-e08f-4571940ec7eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.9.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.0.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.0.post0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.0.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376168 sha256=4783866d7ce9d3d3cd9a6e4fc4323e92bda228bfe7cd912f52da902aef56b981\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wv-mIjSprMjk"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import glob\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# celda necesaria para que el kernel no muera cuando se ejecuta en local\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW8Rj9CTrMjm",
        "outputId": "e656fd0c-dae2-411c-a055-a68f2793581f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 4\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "num_features = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "print(num_features, num_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ClCvXOQhrMjm"
      },
      "outputs": [],
      "source": [
        "# si esta disponible hacer uso de la GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HffwnVtWrMjm"
      },
      "outputs": [],
      "source": [
        "# modelo de la red neuronal\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, num_features, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# creacion de las dos redes neuronales que se van a emplear para aproximar la funcion Q\n",
        "# main: estima los valores de Q en el presente estado y accion\n",
        "# target: estima los valores de Q en el siguiente estado y accion\n",
        "    \n",
        "main_dqn = DQN(num_features, num_actions).to(device)\n",
        "target_dqn = DQN(num_features, num_actions).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(main_dqn.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fqm3VQ23rMjn"
      },
      "outputs": [],
      "source": [
        "# acumular la experiencia para usarla en el entrenamiento\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, device):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "        self.device = device\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        idx = np.random.choice(len(self.buffer), num_samples)\n",
        "        for i in idx:\n",
        "            elem = self.buffer[i]\n",
        "            state, action, reward, next_state, done = elem\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        states = torch.as_tensor(np.array(states), device=self.device)\n",
        "        actions = torch.as_tensor(np.array(actions), device=self.device)\n",
        "        rewards = torch.as_tensor(\n",
        "            np.array(rewards, dtype=np.float32), device=self.device\n",
        "        )\n",
        "        next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
        "        dones = torch.as_tensor(np.array(dones, dtype=np.float32), device=self.device)\n",
        "        return states, actions, rewards, next_states, dones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z4Puau1FrMjn"
      },
      "outputs": [],
      "source": [
        "# elige una accion con probabilidad epsilon en caso contrario elige la mejor accion segun la red neuronal\n",
        "def select_epsilon_greedy_action(state, epsilon):\n",
        "    result = np.random.uniform()\n",
        "    if result < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        qs = main_dqn(state).cpu().data.numpy()\n",
        "        return np.argmax(qs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z0ihSx4FrMjn"
      },
      "outputs": [],
      "source": [
        "# entrenamiento de la red en cada interacción\n",
        "def train_step(states, actions, rewards, next_states, dones):\n",
        "    max_next_qs = target_dqn(next_states).max(-1).values\n",
        "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
        "    q_values = main_dqn(states)\n",
        "    action_maasks = F.one_hot(actions, num_actions)\n",
        "    masked_qs = (action_maasks*q_values).sum(-1)\n",
        "    loss = loss_fn(masked_qs, target.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eN5X_I4trMjo"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters.\n",
        "num_episodes = 5000\n",
        "epsilon = 1.0\n",
        "batch_size = 128\n",
        "discount = 0.99\n",
        "buffer = ReplayBuffer(100000, device=device)\n",
        "cur_frame = 0\n",
        "eps_end = 0.1\n",
        "eps_decay = 0.995"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "riQ6yimqrMjp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cargar los modelos cuando esta disponible la GPU\n",
        "# main_dqn = torch.load('main_dqn.pth')\n",
        "# target_dqn = torch.load('target_dqn.pth')\n",
        "\n",
        "\n",
        "# cargar los modelos cuando no esta disponible la GPU\n",
        "main_dqn.load_state_dict(torch.load('main_dqn.pth', map_location=torch.device('cpu')))\n",
        "target_dqn.load_state_dict(torch.load('main_dqn.pth', map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main_dqn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_dqn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHmym6XVrMjp",
        "outputId": "045c26a0-4294-498e-81c1-87e42fd5793f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward try 1: 280.7255641837435\n",
            "Reward try 2: 276.7286067833994\n",
            "Reward try 3: 307.24980640990975\n",
            "Reward try 4: 296.6506360180057\n",
            "Reward try 5: 276.0892498321335\n",
            "Reward try 6: 258.94640261897223\n",
            "Reward try 7: 286.7131228385335\n",
            "Reward try 8: 282.54531973151313\n",
            "Reward try 9: 292.5681201798651\n",
            "Reward try 10: 237.89599455003\n",
            "Moviepy - Building video ./video/dqn.mp4.\n",
            "Moviepy - Writing video ./video/dqn.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                  \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready ./video/dqn.mp4\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# comporbar el funcionamiento del modelo entrenado con 10 intentos\n",
        "# si la puntuación es superior a 200 se considera que la nave consigue aterrizar\n",
        "\n",
        "path = \"./video/dqn.mp4\"\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "\n",
        "video = VideoRecorder(env, path)\n",
        "\n",
        "for i in range(10):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    ep_reward = 0\n",
        "    while not done:\n",
        "        env.render()\n",
        "        video.capture_frame()\n",
        "        state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
        "        action = select_epsilon_greedy_action(state_in, 0.01)\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Reward try {i+1}: {ep_reward}\")\n",
        "\n",
        "video.close()\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
